#!/usr/bin/env python3
"""GLM-OCR æµ‹è¯•è„šæœ¬ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰"""

import argparse
import time
from PIL import Image
import torch

# æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_PATH = "/Users/lifeng/data/models/GLM-OCR"

# é»˜è®¤æµ‹è¯•å›¾ç‰‡
DEFAULT_IMAGE = "/Users/lifeng/data/pdfs_images/images/batch_06/page_058.png"

# ä»»åŠ¡æç¤ºè¯
TASK_PROMPTS = {
    "text": "Text Recognition:",
    "formula": "Formula Recognition:",
    "table": "Table Recognition:"
}


def test_glm_ocr(image_path: str, task: str = "text", save_result: bool = True):
    """
    æµ‹è¯• GLM-OCR æ¨¡å‹
    
    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        task: ä»»åŠ¡ç±»å‹ (text | formula | table)
        save_result: æ˜¯å¦ä¿å­˜ç»“æœ
    """
    
    print("=" * 80)
    print("GLM-OCR æµ‹è¯•")
    print("=" * 80)
    print(f"\nğŸ“ æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"ğŸ“„ æµ‹è¯•å›¾åƒ: {image_path}")
    print(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {task}")
    print()
    
    start_time = time.time()
    
    # 1. åŠ è½½å›¾åƒ
    print("â³ [1/4] åŠ è½½å›¾åƒ...")
    try:
        image = Image.open(image_path).convert("RGB")
        print(f"âœ… å›¾åƒå¤§å°: {image.size}")
    except Exception as e:
        print(f"âŒ åŠ è½½å›¾åƒå¤±è´¥: {e}")
        return
    
    # 2. åŠ è½½æ¨¡å‹
    print("\nâ³ [2/4] åŠ è½½æ¨¡å‹...")
    try:
        from transformers import AutoProcessor, AutoModelForImageTextToText
        
        # æ£€æµ‹è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        dtype = torch.bfloat16 if device == "cuda" else torch.float32
        
        print(f"   è®¾å¤‡: {device}")
        print(f"   æ•°æ®ç±»å‹: {dtype}")
        
        # åŠ è½½å¤„ç†å™¨
        processor = AutoProcessor.from_pretrained(MODEL_PATH)
        print("âœ… Processor åŠ è½½æˆåŠŸ")
        
        # åŠ è½½æ¨¡å‹ï¼ˆä¸ä½¿ç”¨ device_mapï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡ï¼‰
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_PATH,
            torch_dtype=dtype
        )
        model = model.to(device)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    load_time = time.time() - start_time
    print(f"\nâ±ï¸  åŠ è½½è€—æ—¶: {load_time:.1f} ç§’")
    
    # 3. å‡†å¤‡è¾“å…¥
    print("\nâ³ [3/4] å‡†å¤‡è¾“å…¥...")
    
    # è·å–ä»»åŠ¡æç¤ºè¯
    prompt = TASK_PROMPTS.get(task, "Text Recognition:")
    
    # æ„å»ºæ¶ˆæ¯
    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "url": image_path},
            {"type": "text", "text": prompt}
        ]
    }]
    
    try:
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        ).to(model.device)
        
        # ç§»é™¤ä¸éœ€è¦çš„ token_type_ids
        inputs.pop("token_type_ids", None)
        
        print("âœ… è¾“å…¥å‡†å¤‡å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ è¾“å…¥å‡†å¤‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 4. æ¨ç†
    print(f"\nâ³ [4/4] å¼€å§‹æ¨ç†ï¼ˆ{device.upper()}ï¼‰...")
    print("   æœ€å¤§ç”Ÿæˆ tokens: 8192")
    
    infer_start = time.time()
    
    try:
        with torch.no_grad():
            generated_ids = model.generate(**inputs, max_new_tokens=8192)
        
        infer_time = time.time() - infer_start
        print(f"âœ… æ¨ç†å®Œæˆï¼ˆè€—æ—¶ {infer_time:.1f} ç§’ï¼‰")
        
        # è§£ç ç»“æœ
        output_text = processor.decode(
            generated_ids[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=False
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 80)
        print("è¯†åˆ«ç»“æœ:")
        print("=" * 80)
        print(output_text)
        print("=" * 80)
        
        # ä¿å­˜ç»“æœï¼ˆTXT å’Œ MD æ ¼å¼ï¼‰
        if save_result:
            # ä¿å­˜ä¸º TXT
            txt_file = f'glm_ocr_result_{task}.txt'
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(output_text)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {txt_file}")
            
            # ä¿å­˜ä¸º MD
            md_file = f'glm_ocr_result_{task}.md'
            with open(md_file, 'w', encoding='utf-8') as f:
                # GLM-OCR è¾“å‡ºé€šå¸¸å·²ç»æ˜¯ Markdown æ ¼å¼
                f.write(output_text)
            print(f"ğŸ’¾ Markdown æ ¼å¼å·²ä¿å­˜åˆ°: {md_file}")
        
        # æ€§èƒ½ç»Ÿè®¡
        total_time = time.time() - start_time
        print()
        print("=" * 80)
        print("æ€§èƒ½ç»Ÿè®¡:")
        print("=" * 80)
        print(f"  æ¨¡å‹åŠ è½½: {load_time:.1f} ç§’")
        print(f"  æ¨ç†æ—¶é—´: {infer_time:.1f} ç§’")
        print(f"  æ€»è€—æ—¶:   {total_time:.1f} ç§’")
        print("=" * 80)
        print("\nâœ… æµ‹è¯•å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GLM-OCR æµ‹è¯•è„šæœ¬")
    parser.add_argument(
        "--image",
        type=str,
        default=DEFAULT_IMAGE,
        help="å›¾ç‰‡è·¯å¾„"
    )
    parser.add_argument(
        "--task",
        type=str,
        default="text",
        choices=["text", "formula", "table"],
        help="ä»»åŠ¡ç±»å‹ (text | formula | table)"
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="ä¸ä¿å­˜ç»“æœ"
    )
    
    args = parser.parse_args()
    
    test_glm_ocr(
        image_path=args.image,
        task=args.task,
        save_result=not args.no_save
    )
